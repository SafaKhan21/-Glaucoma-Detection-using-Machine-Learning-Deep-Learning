# -*- coding: utf-8 -*-
"""Untitled6_(15) (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EAIbS_w6AZPcKETPA7XbWjf8huiNB_DV

# Name :Safa Khan
# Id:2110828
# Section:02

# **1- import necessary libraries**
"""

import os
import shutil
import numpy as np
import cv2
import matplotlib.pyplot as plt
from skimage.feature import hog, local_binary_pattern, ORB
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, confusion_matrix

"""# **2- preproccsing  step**"""

# Define the directory where images are stored
image_dir = 'healthy'
diabetic_counter = 1

# Temporary renaming to prevent overwriting
for filename in os.listdir(image_dir):
    old_path = os.path.join(image_dir, filename)
    # Adding a temporary prefix to avoid overwriting
    temp_filename = f'temp_Healthy_{diabetic_counter}.jpg'
    temp_path = os.path.join(image_dir, temp_filename)
    os.rename(old_path, temp_path)
    diabetic_counter += 1

# Final renaming to desired format
diabetic_counter = 1
for filename in os.listdir(image_dir):
    old_path = os.path.join(image_dir, filename)
    # Rename to final format
    new_filename = f'Healthy_{diabetic_counter}.jpg'
    new_path = os.path.join(image_dir, new_filename)
    os.rename(old_path, new_path)
    diabetic_counter += 1

print("Batch renaming complete without overwriting!")

# Define the directory where images are stored
image_dir = 'Glaucoma'
diabetic_counter = 1

# Temporary renaming to prevent overwriting
for filename in os.listdir(image_dir):
    old_path = os.path.join(image_dir, filename)
    # Adding a temporary prefix to avoid overwriting
    temp_filename = f'temp_Glaucoma_{diabetic_counter}.jpg'
    temp_path = os.path.join(image_dir, temp_filename)
    os.rename(old_path, temp_path)
    diabetic_counter += 1

# Final renaming to desired format
diabetic_counter = 1
for filename in os.listdir(image_dir):
    old_path = os.path.join(image_dir, filename)
    # Rename to final format
    new_filename = f'Glaucoma_{diabetic_counter}.jpg'
    new_path = os.path.join(image_dir, new_filename)
    os.rename(old_path, new_path)
    diabetic_counter += 1

print("Batch renaming complete without overwriting!")

# Define the paths for the source folders and the destination folder (to marge the files to one file)
source_folder1 = 'healthy'
source_folder2 = 'Glaucoma'
destination_folder = 'Dataset'

# Create the destination folder if it doesn't exist
os.makedirs(destination_folder, exist_ok=True)

# Copy all files from source_folder1 to destination_folder
for filename in os.listdir(source_folder1):
    source_path = os.path.join(source_folder1, filename)
    dest_path = os.path.join(destination_folder, filename)
    shutil.copy2(source_path, dest_path)

# Copy all files from source_folder2 to destination_folder
for filename in os.listdir(source_folder2):
    source_path = os.path.join(source_folder2, filename)
    dest_path = os.path.join(destination_folder, filename)
    shutil.copy2(source_path, dest_path)

print("Folders merged successfully!")

import os
import cv2
import numpy as np

#  to load images with labels for healthy  retinopathy and glaucoma
def load_images(folder_path, image_size=(128, 128)):
    images = []
    labels = []
    for filename in os.listdir(folder_path):
        filename = filename.strip()  # Remove leading/trailing spaces
        img_path = os.path.join(folder_path, filename)
        img = cv2.imread(img_path)
        if img is not None:
            img = cv2.resize(img, image_size)
            images.append(img)
            # Labeling based on filename
            if 'healthy_' in filename.lower():
                labels.append(0)  # Class 0 for healthy
            elif 'glaucoma_' in filename.lower():
                labels.append(1)  # Class 1 for glaucoma
        # Skip files that don't match expected labels or failed to load

    return np.array(images), np.array(labels)

# Call the function
images, labels = load_images('Dataset')

"""# **3- Feature Extraction step**
# 3.1 HOG
"""

import cv2
import numpy as np
import matplotlib.pyplot as plt
from skimage.feature import hog

def extract_hog_features(images):
    hog_features = []
    hog_images = []  # Store HOG images for visualization
    for img in images:
        # Convert image to grayscale
        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

        # Extract HOG features and the corresponding visualization image
        features, hog_image = hog(gray_img, pixels_per_cell=(8, 8), cells_per_block=(2, 2), block_norm='L2-Hys', visualize=True)
        hog_features.append(features)
        hog_images.append(hog_image)

    return np.array(hog_features), hog_images


hog_features, hog_images = extract_hog_features(images)

"""## 3.1.1 HOG Visualization"""

# Visualize HOG for three sample images
for i in range(3):
    plt.subplot(1, 3, i + 1)
    plt.imshow(hog_images[i], cmap='gray')
    plt.title(f'HOG Visualization: Image {i + 1}')
    plt.axis('off')

plt.show()

"""## 3.2 LBP"""

import numpy as np
from skimage.feature import local_binary_pattern
import matplotlib.pyplot as plt

def extract_lbp_features(images):
    lbp_features = []
    for img in images:
        # Convert image to grayscale
        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

        # Apply Local Binary Pattern (LBP)
        lbp = local_binary_pattern(gray_img, P=8, R=1, method="uniform")

        # Compute the histogram of LBP
        hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 10), range=(0, 9))

        # Normalize the histogram
        hist = hist.astype("float")
        hist /= hist.sum()

        # Append the LBP features for this image
        lbp_features.append(hist)

         # Plot the histogram

    plt.tight_layout()
    plt.show()


    return np.array(lbp_features)


lbp_features = extract_lbp_features(images)

"""# 3.2.1 LBP visualization"""

import numpy as np
import matplotlib.pyplot as plt
from skimage.feature import local_binary_pattern
import cv2
# Function to visualize LBP for three sample images
def visualize_lbp_histograms(images, num_samples=3):
    plt.figure(figsize=(15, 5))

    for i in range(num_samples):
       # Convert to grayscale
        gray_img = cv2.cvtColor(images[i], cv2.COLOR_BGR2GRAY)

        # Compute LBP
        lbp = local_binary_pattern(gray_img, P=8, R=1, method="uniform")

        # Calculate the histogram
        lbp_hist, bins = np.histogram(lbp.ravel(), bins=np.arange(0, 10), range=(0, 9))  # Correct the bin range

        # Normalize the histogram
        lbp_hist = lbp_hist.astype("float")
        lbp_hist /= lbp_hist.sum()


        # Plot the histogram
        plt.subplot(1, num_samples, i + 1)
        plt.bar(range(9),  lbp_hist, width=0.8, align='center')  # Adjust width and alignment for better display
        plt.title(f'LBP Histogram: Image {i + 1}')
        plt.xlabel('LBP Value')
        plt.ylabel('Normalized Frequency')
        plt.xlim([-0.5, 8.5])  # Adjust x limits to match the number of LBP values

    plt.tight_layout()
    plt.show()
visualize_lbp_histograms(images)

"""### 3.3- ORB"""

import cv2
import numpy as np



def enhance_image(img):
    # Apply Gaussian Blur to reduce noise with a valid kernel size
    blurred_img = cv2.GaussianBlur(img, (5, 5), 0)  # Updated to (5, 5)
    return blurred_img
def extract_orb_features(images, max_keypoints=128):
    # Initialize ORB detector
    orb = cv2.ORB_create()
    orb_features_list = []

    for i, img in enumerate(images):
        # Enhance the image before extracting ORB features
        img = enhance_image(img)

        # Detect keypoints and compute descriptors
        keypoints, descriptors = orb.detectAndCompute(img, None)

        # If descriptors are found, process them
        if descriptors is not None:
            # If there are more descriptors than the maximum allowed, truncate them
            if descriptors.shape[0] > max_keypoints:
                descriptors = descriptors[:max_keypoints]
            # If there are fewer descriptors, pad them with zeros
            elif descriptors.shape[0] < max_keypoints:
                padding = np.zeros((max_keypoints - descriptors.shape[0], descriptors.shape[1]), dtype=np.float32)
                descriptors = np.vstack([descriptors, padding])

            orb_features = descriptors.flatten()  # Flatten descriptors into a single vector
        else:
            orb_features = np.zeros((max_keypoints * 32,), dtype=np.float32)  # Placeholder if no descriptors found

        # Append the ORB features for each image
        orb_features_list.append(orb_features)

    return np.array(orb_features_list)



orb_features = extract_orb_features(images)

"""# 3.3.1 ORB Visualization"""

import cv2
import matplotlib.pyplot as plt

def visualize_orb_keypoints(images, orb):
    # Visualize ORB keypoints for each image
    for i, img in enumerate(images):
        # Convert to grayscale if the image is in color
        if len(img.shape) == 3:
            img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        else:
            img_gray = img

        # Detect keypoints and compute descriptors
        keypoints, descriptors = orb.detectAndCompute(img_gray, None)

        # Draw keypoints on the image (in BGR color)
        img_with_keypoints = cv2.drawKeypoints(img, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

        # Convert the image from BGR to RGB for correct color display
        img_with_keypoints_rgb = cv2.cvtColor(img_with_keypoints, cv2.COLOR_BGR2RGB)

        # Display the image with ORB keypoints
        plt.figure(figsize=(10, 6))
        plt.title(f"ORB Keypoints for Image {i + 1}")
        plt.imshow(img_with_keypoints_rgb)  # Show in RGB color
        plt.axis("off")
        plt.show()

# List of image paths
image_paths = ['/content/Dataset/Glaucoma_1.jpg', '/content/Dataset/Glaucoma_237.jpg', '/content/Dataset/Healthy_163.jpg']

# Load images and check if they are loaded correctly
images2 = []
for i, img_path in enumerate(image_paths):
    img = cv2.imread(img_path)
    if img is None:
        print(f"Error: Image {i + 1} at path {img_path} could not be loaded. Please check the file path.")
        exit()  # Terminate if an image fails to load
    images2.append(img)  # Append the image if loaded successfully

# Initialize ORB detector
orb = cv2.ORB_create()

# Visualize ORB keypoints for all the loaded images
visualize_orb_keypoints(images2, orb)

"""## 3.4- Combine all feature extraction"""

combined_features = np.hstack((hog_features, lbp_features, orb_features))

"""# **4- split dataset for training and testing**"""

from sklearn.model_selection import train_test_split
x_train, x_test, Y_train, Y_test = train_test_split(combined_features, labels, test_size=0.2, stratify=labels, random_state=42)

"""## **5- SVM, KNN, and ANN Classifiers (Tranning and Evaluating)**"""

import warnings
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.exceptions import ConvergenceWarning

# Suppress convergence warnings
warnings.filterwarnings("ignore", category=ConvergenceWarning)

# Initialize SVM model and accuracy curves
svm = SVC(kernel='linear', probability=True)
svm_train_accuracy_curve = []
svm_test_accuracy_curve = []

# Training loop for SVM, tracking accuracy
try:
    for epoch in range(1, 11):  # Train for 10 epochs
        # Train the model
        svm.fit(x_train, Y_train)

        # Calculate accuracies
        train_accuracy = accuracy_score(Y_train, svm.predict(x_train))
        test_accuracy = accuracy_score(Y_test, svm.predict(x_test))

        # Append accuracies to the curves
        svm_train_accuracy_curve.append(train_accuracy)
        svm_test_accuracy_curve.append(test_accuracy)

        # Print the accuracy at each epoch
        print(f"Epoch {epoch}: Train Accuracy = {train_accuracy}, Test Accuracy = {test_accuracy}")

    # Get predictions after the training loop
    y_pred_svm = svm.predict(x_test)

    # Print the classification report for predictions
    print("SVM Classification Report:\n", classification_report(Y_test, y_pred_svm))

except Exception as e:
    print("An error occurred during training:", e)

# Plot SVM accuracy curves if they have values
if svm_train_accuracy_curve and svm_test_accuracy_curve:
    plt.plot(range(1, 11), svm_train_accuracy_curve, marker='o', color='blue', label='SVM Training Accuracy')
    plt.plot(range(1, 11), svm_test_accuracy_curve, marker='o', color='orange', label='SVM Validation Accuracy')
    plt.title("SVM Accuracy Over Epochs")
    plt.xlabel("Epochs")
    plt.ylabel("Accuracy")
    plt.legend()
    plt.show()
else:
    print("Error: Accuracy curves are empty.")

import warnings
from sklearn.exceptions import ConvergenceWarning
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report

# Suppress convergence warnings for ANN
warnings.filterwarnings("ignore", category=ConvergenceWarning)

# Assuming x_train, Y_train, x_test, Y_test are already defined

# Train and evaluate KNN
knn = KNeighborsClassifier()
knn_train_accuracy_curve = []
knn_test_accuracy_curve = []

for epoch in range(1, 11):  # Train for 10 epochs
    knn.fit(x_train, Y_train)  # Train KNN

    # Calculate accuracy
    knn_train_accuracy = accuracy_score(Y_train, knn.predict(x_train))
    knn_test_accuracy = accuracy_score(Y_test, knn.predict(x_test))

    # Record accuracy for this epoch
    knn_train_accuracy_curve.append(knn_train_accuracy)
    knn_test_accuracy_curve.append(knn_test_accuracy)

# Get predictions and classification report for KNN
y_pred_knn = knn.predict(x_test)
print("KNN Classification Report:\n", classification_report(Y_test, y_pred_knn))

# Train and evaluate ANN
ann = MLPClassifier(max_iter=1, warm_start=True)  # max_iter=1 for incremental training
ann_train_accuracy_curve = []
ann_val_accuracy_curve = []

for epoch in range(1, 11):  # Train for 10 epochs
    ann.fit(x_train, Y_train)

    # Record training and validation accuracy
    ann_train_accuracy_curve.append(accuracy_score(Y_train, ann.predict(x_train)))
    ann_val_accuracy_curve.append(accuracy_score(Y_test, ann.predict(x_test)))

# Get predictions and classification report for ANN
y_pred_ann = ann.predict(x_test)
print("ANN Classification Report:\n", classification_report(Y_test, y_pred_ann))

# Plot the accuracies for KNN
fig, ax1 = plt.subplots(figsize=(10, 5))  # Plot for KNN accuracy
ax1.plot(range(1, 11), knn_train_accuracy_curve, label='KNN Training Accuracy', linestyle='-', color='blue')
ax1.plot(range(1, 11), knn_test_accuracy_curve, label='KNN Validation Accuracy', linestyle='-', color='orange')
ax1.set_title("KNN Model Accuracy Over Epochs")
ax1.set_xlabel("Epochs")
ax1.set_ylabel("Accuracy")
ax1.legend()

# Adjust layout for KNN plot
plt.tight_layout()
plt.show()

# Plot the accuracies for ANN
fig, ax2 = plt.subplots(figsize=(10, 5))  # Plot for ANN accuracy
ax2.plot(range(1, 11), ann_train_accuracy_curve, label='ANN Training Accuracy', linestyle='-', color='green')
ax2.plot(range(1, 11), ann_val_accuracy_curve, label='ANN Validation Accuracy', linestyle='-', color='red')
ax2.set_title("ANN Model Accuracy Over Epochs")
ax2.set_xlabel("Epochs")
ax2.set_ylabel("Accuracy")
ax2.legend()

# Adjust layout for ANN plot
plt.tight_layout()
plt.show()

"""## 5.1 confusion_matrix pf SVM, KNN,ANN"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

def plot_confusion_matrix(y_true, y_pred, title):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
                xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])
    plt.title(title)
    plt.xlabel('Predicted labels')
    plt.ylabel('True labels')
    plt.show()

# Plotting confusion matrices
plot_confusion_matrix(Y_test, y_pred_svm, "SVM Confusion Matrix")
plot_confusion_matrix(Y_test, y_pred_knn, "KNN Confusion Matrix")
plot_confusion_matrix(Y_test, y_pred_ann, "ANN Confusion Matrix")

"""# **6- CNN with dropout and without dropout**"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
import cv2
import numpy as np
from sklearn.model_selection import train_test_split

# Define constants
num_classes = 2  #  Two classes: Healthy retinopathy and Glaucoma
image_size = (128, 128)  # Ensure images are resized to this size
batch_size = 32
epochs = 10

# Preprocess data for CNN
# Resize images to be compatible with CNN and normalize
images_cnn = np.array([cv2.resize(img, image_size) for img in images])  # Resized images to (128, 128)
images_cnn = images_cnn / 255.0  # Normalize images to [0, 1] (RGB images already have 3 channels)
labels_cnn = to_categorical(labels, num_classes)  # One-hot encoding for labels

# Check if the shape of images_cnn is correct (should be (num_images, 128, 128, 3))
print(images_cnn.shape)

# Split dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(images_cnn, labels_cnn, test_size=0.2, random_state=42)

# Task 2: CNN Model without Dropout
def create_cnn_model(dropout=False):
    model = Sequential([
        # First Conv2D layer with input_shape for RGB images (128, 128, 3)
        Conv2D(32, (3, 3), activation='relu', input_shape=(image_size[0], image_size[1], 3)),
        MaxPooling2D((2, 2)),
        Conv2D(64, (3, 3), activation='relu'),
        MaxPooling2D((2, 2)),
        Conv2D(128, (3, 3), activation='relu'),
        MaxPooling2D((2, 2)),
        Flatten(),
        Dense(128, activation='relu'),
    ])
    if dropout:
        model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))  # Output layer
    return model

# CNN without Dropout
model_no_dropout = create_cnn_model(dropout=False)
model_no_dropout.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
history_no_dropout = model_no_dropout.fit(X_train, y_train, validation_data=(X_test, y_test),
                                           epochs=epochs, batch_size=batch_size)

# CNN with Dropout
model_with_dropout = create_cnn_model(dropout=True)
model_with_dropout.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
history_with_dropout = model_with_dropout.fit(X_train, y_train, validation_data=(X_test, y_test),
                                              epochs=epochs, batch_size=batch_size)

# Plotting training and validation curves
def plot_training_curves(history, title):
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title(f'{title} Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

plot_training_curves(history_no_dropout, "CNN without Dropout")
plot_training_curves(history_with_dropout, "CNN with Dropout")

# Part 3: Model Evaluation and Comparison
# Predict and evaluate CNN without Dropout
y_pred_no_dropout = model_no_dropout.predict(X_test).argmax(axis=1)
y_true = y_test.argmax(axis=1)
conf_matrix_no_dropout = confusion_matrix(y_true, y_pred_no_dropout)

# Predict and evaluate CNN with Dropout
y_pred_with_dropout = model_with_dropout.predict(X_test).argmax(axis=1)
conf_matrix_with_dropout = confusion_matrix(y_true, y_pred_with_dropout)

# Display confusion matrices
def plot_confusion_matrix(cm, title):
    plt.figure(figsize=(3, 3))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
    plt.title(f'Confusion Matrix for {title}')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

plot_confusion_matrix(conf_matrix_no_dropout, "CNN without Dropout")
plot_confusion_matrix(conf_matrix_with_dropout, "CNN with Dropout")

# Classification Reports
print("Classification Report for CNN without Dropout:\n", classification_report(y_true, y_pred_no_dropout))
print("Classification Report for CNN with Dropout:\n", classification_report(y_true, y_pred_with_dropout))

# T3.2 Dropout Impact Analysis
print("Training Accuracy without Dropout:", max(history_no_dropout.history['accuracy']))
print("Validation Accuracy without Dropout:", max(history_no_dropout.history['val_accuracy']))
print("Training Accuracy with Dropout:", max(history_with_dropout.history['accuracy']))
print("Validation Accuracy with Dropout:", max(history_with_dropout.history['val_accuracy']))

"""# **5.1- cost function for CNN**"""

import matplotlib.pyplot as plt

# 1. Plot Loss Curve for CNN without Dropout
plt.figure(figsize=(8, 6))
plt.plot(history_no_dropout.history['loss'], label="Training Loss (No Dropout)")
plt.plot(history_no_dropout.history['val_loss'], label="Validation Loss (No Dropout)")
plt.title("Training and Validation Loss (CNN Without Dropout)")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()

# 2. Plot Loss Curve for CNN with Dropout
plt.figure(figsize=(8, 6))
plt.plot(history_with_dropout.history['loss'], label="Training Loss (With Dropout)")
plt.plot(history_with_dropout.history['val_loss'], label="Validation Loss (With Dropout)")
plt.title("Training and Validation Loss (CNN With Dropout)")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()

"""# 1- model with dropout
## The model with Dropout demonstrated more stable performance on the validation set, indicating reduced overfitting.

# 2- model without dropout

## The model showed signs of overfitting, with high training accuracy but lower validation accuracy.

# **6.3- Confusion Matrix for all models**
"""

# Import necessary libraries
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
import numpy as np

# Define a helper function to plot a confusion matrix on a given axis
def plot_confusion_matrix(ax, y_true, y_pred, title):
    # Convert one-hot encoded labels to class indices (e.g., 0 or 1)
    y_true_class = np.argmax(y_true, axis=1) if len(y_true.shape) > 1 else y_true
    y_pred_class = np.argmax(y_pred, axis=1) if len(y_pred.shape) > 1 else y_pred

    # Compute confusion matrix
    cm = confusion_matrix(y_true_class, y_pred_class)

    # Plot confusion matrix using seaborn heatmap on the provided axis
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False, ax=ax)
    ax.set_title(title)
    ax.set_xlabel("Predicted Label")
    ax.set_ylabel("True Label")


fig, axes = plt.subplots(1, 5, figsize=(20, 4))  # 1 row, 5 columns

# Plot each confusion matrix in its respective subplot
plot_confusion_matrix(axes[0], y_test, y_pred_svm, "SVM Confusion Matrix")
plot_confusion_matrix(axes[1], y_test, y_pred_knn, "KNN Confusion Matrix")
plot_confusion_matrix(axes[2], y_test, y_pred_ann, "ANN Confusion Matrix")
plot_confusion_matrix(axes[3], y_test, y_pred_no_dropout, "CNN (No Dropout)")
plot_confusion_matrix(axes[4], y_test, y_pred_with_dropout, "CNN (With Dropout)")

# Adjust layout for better spacing
plt.tight_layout()
plt.show()

"""#	T3.2-Dropout **Impact**
## The introduction of Dropout layers reduced overfitting and improved the CNN model's ability to generalize to unseen data, as seen in the improved validation performance and more balanced confusion matrix

# **8- Comparison in in terms of accuracy, precision, and recall**
"""

import numpy as np
from sklearn.metrics import classification_report, confusion_matrix

# Convert y_test to class indices if it's one-hot encoded
if len(y_test.shape) > 1:
    y_test_classes = np.argmax(y_test, axis=1)
else:
    y_test_classes = y_test

# SVM Evaluation
y_pred_svm = svm.predict(x_test)
print("SVM Classification Report:\n", classification_report(Y_test, y_pred_svm))

# KNN Evaluation
y_pred_knn = knn.predict(x_test)
print("KNN Classification Report:\n", classification_report(Y_test, y_pred_knn))

# ANN Evaluation
y_pred_ann = ann.predict(x_test)
print("ANN Classification Report:\n", classification_report(Y_test, y_pred_ann))

# CNN without Dropout Evaluation
y_pred_no_dropout = model_no_dropout.predict(X_test).argmax(axis=1)
print("CNN without Dropout Classification Report:\n", classification_report(y_test_classes, y_pred_no_dropout))

# CNN with Dropout Evaluation
y_pred_with_dropout = model_with_dropout.predict(X_test).argmax(axis=1)
print("CNN with Dropout Classification Report:\n", classification_report(y_test_classes, y_pred_with_dropout))



import matplotlib.pyplot as plt
from sklearn.metrics import classification_report

# Create a figure with 1 row and 3 columns for the SVM, KNN, and ANN plots
fig, axes = plt.subplots(1, 3, figsize=(12, 4))  # Reduced size

# Plot the accuracies for SVM
axes[0].plot(range(1, 11), svm_train_accuracy_curve, linestyle='-', color='blue', label='SVM Training Accuracy')
axes[0].plot(range(1, 11), svm_test_accuracy_curve, linestyle='-', color='orange', label='SVM Validation Accuracy')
axes[0].set_title("SVM Accuracy Over Epochs")
axes[0].set_xlabel("Epochs")
axes[0].set_ylabel("Accuracy")
axes[0].legend()

# Plot the accuracies for KNN
axes[1].plot(range(1, 11), knn_train_accuracy_curve, label='KNN Training Accuracy', linestyle='-', color='blue')
axes[1].plot(range(1, 11), knn_test_accuracy_curve, label='KNN Validation Accuracy', linestyle='-', color='orange')
axes[1].set_title("KNN Model Accuracy Over Epochs")
axes[1].set_xlabel("Epochs")
axes[1].set_ylabel("Accuracy")
axes[1].legend()

# Plot the accuracies for ANN
axes[2].plot(range(1, 11), ann_train_accuracy_curve, label='ANN Training Accuracy', linestyle='-', color='blue')
axes[2].plot(range(1, 11), ann_val_accuracy_curve, label='ANN Validation Accuracy', linestyle='-', color='orange')
axes[2].set_title("ANN Model Accuracy Over Epochs")
axes[2].set_xlabel("Epochs")
axes[2].set_ylabel("Accuracy")
axes[2].legend()

# Adjust layout to prevent overlap
plt.tight_layout()

# Show the plots
plt.show()

plot_training_curves(history_no_dropout, "CNN without Dropout")  # Smaller size
plot_training_curves(history_with_dropout, "CNN with Dropout")

"""**Best Model**: CNN with Dropout has the accuracy (90%) and consistently high precision, recall, and F1-scores.

 **Improvement over Simpler Models**: Both CNN models (with and without dropout) outperform the simpler models (SVM, KNN, ANN), likely due to the greater capacity of CNNs to capture complex patterns in the data and do no need to extract features.

  **Impact of Dropout:** Adding dropout slightly improves the performance, likely due to reduced overfitting, as evidenced by the slight increase in accuracy and better-balanced precision and recall for both classes.


"""